{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单头注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# 构建字典\n",
    "sentence = 'Life is short, eat dessert first'\n",
    "\n",
    "# 简单起见，我们只用句子中的单词作为词典\n",
    "dc = {w:i for i, w in enumerate(sorted(sentence.replace(',', '').split()))}\n",
    "\n",
    "\n",
    "# sentence -> dict index\n",
    "sentence_int = torch.tensor([dc[w] for w in sentence.replace(',', '').split()]) # (1, seq_len)\n",
    "\n",
    "# dict index -> embedding\n",
    "d = 16 # embedding dimension\n",
    "embed = torch.nn.Embedding(len(sentence_int), d)\n",
    "embedded_sentence = embed(sentence_int).detach() # (seq_len, d)\n",
    "\n",
    "# 定义 W_q, W_k, W_v\n",
    "d_q, d_k, d_v = 24, 24, 28 # d_q = d_k\n",
    "W_q = torch.nn.Parameter(torch.randn(d_q, d))\n",
    "W_k = torch.nn.Parameter(torch.randn(d_k, d))\n",
    "W_v = torch.nn.Parameter(torch.randn(d_v, d))\n",
    "\n",
    "# 句子嵌入乘以 W_q, W_k, W_v 得到 Q, K, V\n",
    "querys = (W_q @ embedded_sentence.T).T # (seq_len, d_q)\n",
    "keys = (W_k @ embedded_sentence.T).T # (seq_len, d_k)\n",
    "values = (W_v @ embedded_sentence.T).T # (seq_len, d_v)\n",
    "\n",
    "# Q, K 点乘得到 attention weights\n",
    "omega = querys @ keys.T # (seq_len, seq_len)\n",
    "attention_weights = F.softmax(omega / d_k**0.5, dim=0) # (seq_len, seq_len)\n",
    "\n",
    "# attention weights 乘以 V 得到 context vector\n",
    "context_vector = attention_weights @ values # (seq_len, d_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多头注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "h = 3 # number of heads\n",
    "\n",
    "# 构建字典\n",
    "sentence = 'Life is short, eat dessert first'\n",
    "\n",
    "# 简单起见，我们只用句子中的单词作为词典\n",
    "dc = {w:i for i, w in enumerate(sorted(sentence.replace(',', '').split()))}\n",
    "\n",
    "\n",
    "# sentence -> dict index\n",
    "sentence_int = torch.tensor([dc[w] for w in sentence.replace(',', '').split()]) # (1, seq_len)\n",
    "\n",
    "# dict index -> 单头 embedding\n",
    "d = 16 # embedding dimension\n",
    "embed = torch.nn.Embedding(len(sentence_int), d)\n",
    "embedded_sentence = embed(sentence_int).detach() # (seq_len, d)\n",
    "\n",
    "# 将单头输入复制 h 次\n",
    "stacked_inputs = embedded_sentence.T.repeat(h, 1, 1).permute(0, 2, 1) # (h, seq_len, d)\n",
    "\n",
    "# 定义 W_q, W_k, W_v\n",
    "d_q, d_k, d_v = 24, 24, 28 # d_q = d_k\n",
    "W_q = torch.nn.Parameter(torch.randn(h, d_q, d))\n",
    "W_k = torch.nn.Parameter(torch.randn(h, d_k, d))\n",
    "W_v = torch.nn.Parameter(torch.randn(h, d_v, d))\n",
    "\n",
    "# 句子嵌入乘以 W_q, W_k, W_v 得到 Q, K, V\n",
    "querys = (W_q @ stacked_inputs.transpose(1, 2)).transpose(1, 2) # (h, seq_len, d_q)\n",
    "keys = (W_k @ stacked_inputs.transpose(1, 2)).transpose(1, 2) # (h, seq_len, d_k)\n",
    "values = (W_v @ stacked_inputs.transpose(1, 2)).transpose(1, 2) # (h, seq_len, d_v)\n",
    "\n",
    "# Q, K 点乘得到 attention weights\n",
    "omegas = torch.matmul(querys, keys.transpose(1, 2)) # (h, seq_len, seq_len)\n",
    "attention_weights = F.softmax(omega / d_k**0.5, dim=0) # (h, seq_len, seq_len)\n",
    "\n",
    "# attention weights 乘以 V 得到 context vector\n",
    "context_vector = torch.matmul(attention_weights, values) # (h, seq_len, d_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cross-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
